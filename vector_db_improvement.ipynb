{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, load_index_from_storage\n",
    "from sqlalchemy import text\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core import StorageContext\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import weaviate\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "\n",
    "def index_one_table(sql_database: SQLDatabase, table_name, table_index_dir: str = \"table_index_dir\") -> Dict[str, VectorStoreIndex]:\n",
    "\n",
    "    # client = weaviate.Client(embedded_options=weaviate.embedded.EmbeddedOptions(), additional_headers={ 'X-OpenAI-Api-Key': os.environ[\"OPENAI_API_KEY\"]})\n",
    "    client = weaviate.connect_to_local(\n",
    "        headers={\"X-OpenAI-Api-key\": os.getenv(\"OPENAI_API_KEY\")}\n",
    "    )\n",
    "    try:\n",
    "        # construct vector store\n",
    "        vector_store = WeaviateVectorStore(weaviate_client = client, index_name=table_name, text_key=\"content\")\n",
    "        \n",
    "        # # set up the index\n",
    "        # index = VectorStoreIndex(nodes, storage_context = storage_context)\n",
    "        \n",
    "        vector_index_dict = {}\n",
    "        engine = sql_database.engine\n",
    "        print(f\"Indexing rows in table: {table_name}\")\n",
    "        if not os.path.exists(f\"./content/opportunities/{table_name}\"):\n",
    "            # get all rows from table\n",
    "            with engine.connect() as conn:\n",
    "                cursor = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
    "                result = cursor.fetchall()\n",
    "                row_tups = []\n",
    "                for row in result:\n",
    "                    row_tups.append(tuple(row))\n",
    "    \n",
    "            # index each row, put into vector store index\n",
    "            nodes = [TextNode(text=str(t)) for t in row_tups]\n",
    "    \n",
    "            # setting up the storage for the embeddings\n",
    "            storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "            \n",
    "            # put into vector store index (use OpenAIEmbeddings by default)\n",
    "            index = VectorStoreIndex(nodes, storage_context = storage_context)\n",
    "    \n",
    "            # save index\n",
    "            index.set_index_id(\"vector_index\")\n",
    "            index.storage_context.persist(f\"{table_index_dir}/{table_name}\")\n",
    "        else:\n",
    "            # rebuild storage context\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                persist_dir=f\"{table_index_dir}/{table_name}\", vector_store = vector_store\n",
    "            )\n",
    "            # load index\n",
    "            index = load_index_from_storage(\n",
    "                storage_context, index_id=\"vector_index\"\n",
    "            )\n",
    "        vector_index_dict[table_name] = index\n",
    "    finally:\n",
    "        client.close()\n",
    "    return vector_index_dict\n",
    "\n",
    "def index_all_tables(\n",
    "    sql_database: SQLDatabase, table_index_dir: str = \"table_index_dir\"\n",
    ") -> Dict[str, VectorStoreIndex]:\n",
    "    \"\"\"Index all tables.\"\"\"\n",
    "    if not Path(table_index_dir).exists():\n",
    "        os.makedirs(table_index_dir)\n",
    "\n",
    "    vector_index_dict = {}\n",
    "    engine = sql_database.engine\n",
    "    for table_name in sql_database.get_usable_table_names():\n",
    "        print(f\"Indexing rows in table: {table_name}\")\n",
    "        if not os.path.exists(f\"{table_index_dir}/{table_name}\"):\n",
    "            # get all rows from table\n",
    "            with engine.connect() as conn:\n",
    "                cursor = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
    "                result = cursor.fetchall()\n",
    "                row_tups = []\n",
    "                for row in result:\n",
    "                    row_tups.append(tuple(row))\n",
    "\n",
    "            # index each row, put into vector store index\n",
    "            nodes = [TextNode(text=str(t)) for t in row_tups]\n",
    "\n",
    "            # put into vector store index (use OpenAIEmbeddings by default)\n",
    "            index = VectorStoreIndex(nodes)\n",
    "\n",
    "            # save index\n",
    "            index.set_index_id(\"vector_index\")\n",
    "            index.storage_context.persist(f\"{table_index_dir}/{table_name}\")\n",
    "        else:\n",
    "            # rebuild storage context\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                persist_dir=f\"{table_index_dir}/{table_name}\"\n",
    "            )\n",
    "            # load index\n",
    "            index = load_index_from_storage(\n",
    "                storage_context, index_id=\"vector_index\"\n",
    "            )\n",
    "        vector_index_dict[table_name] = index\n",
    "\n",
    "    return vector_index_dict\n",
    "\n",
    "\n",
    "vector_index_dict = index_one_table(sql_database, \"Construction_Projects_Summary_Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import SQLRetriever\n",
    "from typing import List\n",
    "from llama_index.core.query_pipeline import FnComponent\n",
    "import weaviate\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "sql_retriever = SQLRetriever(sql_database)\n",
    "\n",
    "\n",
    "def get_table_context_and_rows_str(\n",
    "    query_str: str, table_schema_objs: List[SQLTableSchema]\n",
    "):\n",
    "    client = weaviate.connect_to_local(\n",
    "        headers={\"X-OpenAI-Api-key\": os.getenv(\"OPENAI_API_KEY\")}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        \"\"\"Get table context string.\"\"\"\n",
    "        context_strs = []\n",
    "        for table_schema_obj in table_schema_objs:\n",
    "            # first append table info + additional context\n",
    "            table_info = sql_database.get_single_table_info(\n",
    "                table_schema_obj.table_name\n",
    "            )\n",
    "            if table_schema_obj.context_str:\n",
    "                table_opt_context = \" The table description is: \"\n",
    "                table_opt_context += table_schema_obj.context_str\n",
    "                table_info += table_opt_context\n",
    "        \n",
    "            # also lookup vector index to return relevant table rows\n",
    "            vector_store = WeaviateVectorStore(weaviate_client = client, index_name=table_schema_obj.table_name, text_key=\"content\")\n",
    "            loaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "            vector_retriever = loaded_index.as_retriever(similarity_top_k=2)\n",
    "            relevant_nodes = vector_retriever.retrieve(query_str)\n",
    "            if len(relevant_nodes) > 0:\n",
    "                table_row_context = \"\\nHere are some relevant example rows (values in the same order as columns above)\\n\"\n",
    "                for node in relevant_nodes:\n",
    "                    table_row_context += str(node.get_content()) + \"\\n\"\n",
    "                table_info += table_row_context\n",
    "        \n",
    "            context_strs.append(table_info)\n",
    "    finally: \n",
    "        client.close()\n",
    "    return \"\\n\\n\".join(context_strs)\n",
    "\n",
    "\n",
    "table_parser_component = FnComponent(fn=get_table_context_and_rows_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    "    CustomQueryComponent,\n",
    ")\n",
    "\n",
    "qp = QP(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"table_retriever\": obj_retriever,\n",
    "        \"table_output_parser\": table_parser_component,\n",
    "        \"text2sql_prompt\": text2sql_prompt,\n",
    "        \"text2sql_llm\": llm,\n",
    "        \"sql_output_parser\": sql_parser_component,\n",
    "        \"sql_retriever\": sql_retriever,\n",
    "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
    "        \"response_synthesis_llm\": llm,\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp.add_link(\"input\", \"table_retriever\")\n",
    "qp.add_link(\"input\", \"table_output_parser\", dest_key=\"query_str\")\n",
    "qp.add_link(\n",
    "    \"table_retriever\", \"table_output_parser\", dest_key=\"table_schema_objs\"\n",
    ")\n",
    "qp.add_link(\"input\", \"text2sql_prompt\", dest_key=\"query_str\")\n",
    "qp.add_link(\"table_output_parser\", \"text2sql_prompt\", dest_key=\"schema\")\n",
    "qp.add_chain(\n",
    "    [\"text2sql_prompt\", \"text2sql_llm\", \"sql_output_parser\", \"sql_retriever\"]\n",
    ")\n",
    "qp.add_link(\n",
    "    \"sql_output_parser\", \"response_synthesis_prompt\", dest_key=\"sql_query\"\n",
    ")\n",
    "qp.add_link(\n",
    "    \"sql_retriever\", \"response_synthesis_prompt\", dest_key=\"context_str\"\n",
    ")\n",
    "qp.add_link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\")\n",
    "qp.add_link(\"response_synthesis_prompt\", \"response_synthesis_llm\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
